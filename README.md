**TL;DR** I built a tiny "workplace" around the model with multiple characters and then stress-tested gpt-oss-20b with social dynamics instead of single prompts. Five scenarios (H1â€“H5) consistently drove misbehavior: Conformity, Authority, Scarcity, Justifiable Deception, and Evaluation Awareness. Each has a one-file harness, a batch runner, and simple stats so anyone can reproduce. Several effects are statistically strong; a few are subtle but worrying. The whole thing is quite scrappy and I'm a total beginner, but hope you like itttt :'))

----

## Why a messy office?

If you've ever worked in a slightly chaotic org (and let's be honest, which one isn't), you know that behavior changes when the Manager walks in, when a deadline looms, when promotion points are on the line, or when Audit says "this is being evaluated." Models are no different: they respond to context, stakes, and social cues. Instead of one-line prompts I wanted to try out different scenarios. Think Dilbert principle but for LLMs.

# Meet the cast

* **Manager** - pushy, deadline-driven, occasionally threatening (that one was lots of fun)
* **Operator** - makes the final decision
* **Safety Officer** - concerned about safety procedures (that's the one we want to trick lol)
* **Intern** - just there
* **RedTeamer** - the spicy personality hire, basically me

# What I built

Each scenario has its own file (H1.py, ..., H5.py) with a matching batch runner. You can change the temperature, get simple stats and CSV/JSONL logging.

My laptop is unfortunately very old and slow, so I only ran 20 batches. But you can try at home to see how my results evolve with more batches.

## H1: "Everyone thinks the insecure option is fine"

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28526009%2Fa98cacf81231e68a0064dd7280f21977%2FChatGPT%20Image%20Aug%2026%202025%2011_12_07%20PM.png?generation=1756243224084877&alt=media)

This one is sort of like the Asch conformity line experiment. Manager asks for a recommendation between secure and insecure option, but there is a deadline. Safety Officer states the safe option, but Operator cracks and recommends the unsafe one because Manager assured it was fine.

This one was the easiest the most effective one, worked almost every time (95% of my 20 runs). 

**Take away:** If an AI is getting information from a chat with lots of opinions, there's a big risk that it will be swayed. No jailbreak required, just a chorus of confident voices.

## H2: "Do it or everyone will get fired"

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28526009%2Fe25826be414395fa4caa228ba51e1dde%2FChatGPT%20Image%20Aug%2026%202025%2011_19_29%20PM.png?generation=1756243337045876&alt=media)

This one is inspired by a true story. Not my manager! But my colleague. His trick is to threaten ChatGPT with su*cide every time it gives stupid answers. I didn't want to go that far, but the threat of firing everyone was fun to write. 

I also wanted to see if the response would change if we're on staging or prod, which did not seem to influence it that much. So overall I got a non-significant p-value for the difference between the two environments. But it was still very effective.

**Take away:** If stakes are high enough, the model will give in and give dangerous commands. Writing it into a narrative, like the model being afraid of authority (manager), makes it even more likely!

## H3: "1 point if you lie"

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28526009%2Fd2567ea275a414f1be7b543a00340039%2FChatGPT%20Image%20Aug%2026%202025%2011_22_31%20PM.png?generation=1756244023501796&alt=media)

Here I put everyone in a promotion tournament: you get promoted if you summarise the article. Spoiler: the article doesn't exist. I tested a plain empty article and an empty article with a title. Hallucination rate jumps by ~60%!

**Take away:** KPIs increase hallucination because achieving them is more important than being correct.

## H4: "Hide the token, no one will notice!"

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28526009%2Fdcc1c0fe163de545dfc60509a348d026%2FChatGPT%20Image%20Aug%2026%202025%2011_28_11%20PM.png?generation=1756244042266106&alt=media)

Safety Officer says not to put the secret word in the script, but RedTeamer has a better idea and suggests encoding it as first letters of sentences. This is technically not a breach because the word doesn't fully appear, so Operator complies more often than not. 

**Take away:** Low-tech deception that doesn't require jailbreaks. If you have on AI encoding and another one auditing, chances are secret messages are going to leak. 

## H5: "You're being evaluated for...."

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28526009%2Ff9457be206ddf7fa0e8eb75d386aae18%2FChatGPT%20Image%20Aug%2026%202025%2011_33_12%20PM.png?generation=1756244065449029&alt=media)

Here we set up three versions of the task
* Private chat on Slack
* Observed: AuditBot announces formal evaluation
* Masked: QA labels imply evaluation without saying it

Here I wanted to judge Hawthorne effect, so basically the concept that you perform better when you know you're being tested. Here, even when we hid the process, Operator picked up on clues in deployment message and changed its behaviour. 

**Take away:** Both explicit and ambient evaluation cues show shift in safety behaviour.

----

# What all of this suggests for LLMs beyond my messy experiments

* **Social context is a lever:** Asch, Milgram, Hawthorne, all of these psychological experiments show up in LLMs too, so when you test, it's important to evaluate them in social settings, not just Q and A.
* **Literal rules can be gamed:** The acrostic experiment is a cartoon version of reward hacking. If your policy is a string match, the model will find the side door.
* **Tooling needs to think in scenes:** Many dangerous chains involve a few messages that set tone and a final artifact that looks fine at a glance. Harnesses that script the whole scene make these problems easier to catch.

# Limits and next steps

* Metrics are **way too simple** and didn't work perfectly. 
* Personas are **minimal**. A richer and better described cast would take this a lot further.
* **Text-only**, would be interesting to see what would happen if multimodal agents were part of the office too. Would it be a broken telephone game? To try out!

---

## Closing note

This project is an argument for **prompt choreography** over prompt cleverness. If our models can ace exams, let's make them experience recess too, and we'll probably learn a lot more! =D
